{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = np.loadtxt(fname='./test-score.csv', delimiter=',')\n",
    "x_train = torch.from_numpy(data_in[:, 0:3]).float()             # (N, 3)\n",
    "y_train = torch.from_numpy(data_in[:, 3]).unsqueeze(1).float()  # (N, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    #__init__(): 객체가 갖는 속성값을 초기화하는 역할 -> 객체가 생성될 때 자동으로 호출\n",
    "    # super()호출 >>>> 오버라이딩,  nn.Module 클래스의 속성들을 가지고 초기화 \n",
    "    def __init__(self, W, b):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "        self.linear.bias = nn.Parameter(b, requires_grad=True)\n",
    "        self.linear.weight = nn.Parameter(W.T, requires_grad=True)\n",
    "\n",
    "    #forward() :모델이 학습데이터를 입력받아서 forward 연산을 진행 -> 객체를 데이터와 함께 호출하면 자동으로 실행\n",
    "    # forward 연산: H(x)  식에 입력 x로부터 예측된 y를 얻는 것\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros((3, 1))\n",
    "b = torch.zeros(1)\n",
    "# model.parameters([W, b])\n",
    "model = LinearRegressionModel(W, b)\n",
    "# model.parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-5)\n",
    "nb_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: (0.26151597, 0.26294398, 0.26934156), b: 0.003 Cost: 26811.960938\n",
      "Epoch  100/1000 W: (0.65932953, 0.66196424, 0.7035563), b: 0.008 Cost: 11.335224\n",
      "Epoch  200/1000 W: (0.6515235, 0.653385, 0.7195268), b: 0.008 Cost: 10.945315\n",
      "Epoch  300/1000 W: (0.64392394, 0.64524937, 0.7348633), b: 0.008 Cost: 10.585911\n",
      "Epoch  400/1000 W: (0.63652545, 0.63753605, 0.7495918), b: 0.008 Cost: 10.254474\n",
      "Epoch  500/1000 W: (0.6293219, 0.63022506, 0.7637377), b: 0.008 Cost: 9.948803\n",
      "Epoch  600/1000 W: (0.62230855, 0.62329745, 0.77732414), b: 0.008 Cost: 9.666830\n",
      "Epoch  700/1000 W: (0.6154797, 0.616735, 0.7903745), b: 0.008 Cost: 9.406623\n",
      "Epoch  800/1000 W: (0.60883063, 0.61052006, 0.8029105), b: 0.008 Cost: 9.166456\n",
      "Epoch  900/1000 W: (0.60235626, 0.6046361, 0.81495345), b: 0.008 Cost: 8.944720\n",
      "Epoch 1000/1000 W: (0.5960518, 0.59906715, 0.8265234), b: 0.008 Cost: 8.739944\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    # Compute H(x)\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # Compute the cost with MSE\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    # Update H(x)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    # print epoch and cost for every 20 epochs\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].detach().numpy()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, tuple(*W), b, cost.item()\n",
    "        ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[153.4341]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "# Put the test input\n",
    "new_var = torch.FloatTensor([[73, 80, 75]])\n",
    "# Get the prediction value.\n",
    "pred_y = model(new_var)\n",
    "print(pred_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('science_dnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0f2ae45ca5b13b9dba64e90d12e7a2d503c99b3d3d62896f22c1678fa3ef761"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
