{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ H(x) = Wx + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ cost(W, b) = \\frac{1}{m} \\sum^m_{i=1} \\left( H(x^{(i)}) - y^{(i)} \\right)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2557c2109b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  remark\n",
    "torch.manual_seed()를 하는 이유: torch.manual_seed()를 사용한 프로그램의 결과는 다른 컴퓨터에서 실행시켜도 동일한 결과를 얻을 수 있음. torch.manual_seed()는 난수 발생 순서와 값을 동일하게 보장. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시드가 3일 때\n",
      "tensor([0.0043])\n",
      "tensor([0.1056])\n",
      "랜덤 시드가 5일 때\n",
      "tensor([0.8303])\n",
      "tensor([0.1261])\n",
      "랜덤 시드가 다시 3일 때\n",
      "tensor([0.0043])\n",
      "tensor([0.1056])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "print('랜덤 시드가 3일 때')\n",
    "for i in range(1,3):\n",
    "  print(torch.rand(1))\n",
    "\n",
    "torch.manual_seed(5)\n",
    "print('랜덤 시드가 5일 때')\n",
    "for i in range(1,3):\n",
    "  print(torch.rand(1))\n",
    "\n",
    "torch.manual_seed(3)\n",
    "print('랜덤 시드가 다시 3일 때')\n",
    "for i in range(1,3):\n",
    "  print(torch.rand(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.],\n",
      "        [4.],\n",
      "        [6.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros(1, requires_grad=True) # \"requires_grad=True\" 학습 할 것이라고 알려줍시다.\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "b = torch.zeros(1, requires_grad=True) # \"requires_grad=True\" 학습 할 것이라고 알려줍시다.\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ H(x) = Wx + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = x_train * W + b\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ cost(W, b) = \\frac{1}{m} \\sum^m_{i=1} \\left( H(x^{(i)}) - y^{(i)} \\right)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.],\n",
      "        [-4.],\n",
      "        [-6.]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis - y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.],\n",
      "        [16.],\n",
      "        [36.]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print((hypothesis - y_train) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W =  tensor([0.], requires_grad=True)\n",
      "b =  tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD([W, b], lr=0.01) # learning rate = 0.01\n",
    "print(\"W = \", W)\n",
    "print(\"b = \", b)\n",
    "# 아직 아무일도 일어나지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음과 같이 지정한다:\n",
    "\n",
    "zero_grad 로 gradient 초기화\n",
    "\n",
    "backward 로 gradient 계산\n",
    "\n",
    "step 으로 update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W =  tensor([0.], requires_grad=True)\n",
      "b =  tensor([0.], requires_grad=True)\n",
      "W =  tensor([0.], requires_grad=True)\n",
      "b =  tensor([0.], requires_grad=True)\n",
      "W =  tensor([0.1867], requires_grad=True)\n",
      "b =  tensor([0.0800], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "print(\"W = \", W)\n",
    "print(\"b = \", b)\n",
    "cost.backward()\n",
    "print(\"W = \", W)\n",
    "print(\"b = \", b)\n",
    "optimizer.step()\n",
    "print(\"W = \", W)\n",
    "print(\"b = \", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18666668236255646\n",
      "0.07999999821186066\n"
     ]
    }
   ],
   "source": [
    "# 값만 보고싶다면 item()을 사용\n",
    "print(W.item())\n",
    "print(b.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the hypothesis is now better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2667],\n",
      "        [0.4533],\n",
      "        [0.6400]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = x_train * W + b\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.7710, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Full Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, we will be training on the dataset for multiple epochs. This can be done simply with loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.187, b: 0.080 Cost: 18.666666\n",
      "Epoch  100/1000 W: 1.746, b: 0.578 Cost: 0.048171\n",
      "Epoch  200/1000 W: 1.800, b: 0.454 Cost: 0.029767\n",
      "Epoch  300/1000 W: 1.843, b: 0.357 Cost: 0.018394\n",
      "Epoch  400/1000 W: 1.876, b: 0.281 Cost: 0.011366\n",
      "Epoch  500/1000 W: 1.903, b: 0.221 Cost: 0.007024\n",
      "Epoch  600/1000 W: 1.924, b: 0.174 Cost: 0.004340\n",
      "Epoch  700/1000 W: 1.940, b: 0.136 Cost: 0.002682\n",
      "Epoch  800/1000 W: 1.953, b: 0.107 Cost: 0.001657\n",
      "Epoch  900/1000 W: 1.963, b: 0.084 Cost: 0.001024\n",
      "Epoch 1000/1000 W: 1.971, b: 0.066 Cost: 0.000633\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark \n",
    "optimizer.zero_grad()가 필요한 이유: 파이토치는 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값에 누적시키는 특징이 있습니다. \n",
    "\n",
    "예를 들어봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수식을 w로 미분한 값 : 2.0\n",
      "수식을 w로 미분한 값 : 4.0\n",
      "수식을 w로 미분한 값 : 6.0\n",
      "수식을 w로 미분한 값 : 8.0\n",
      "수식을 w로 미분한 값 : 10.0\n",
      "수식을 w로 미분한 값 : 12.0\n",
      "수식을 w로 미분한 값 : 14.0\n",
      "수식을 w로 미분한 값 : 16.0\n",
      "수식을 w로 미분한 값 : 18.0\n",
      "수식을 w로 미분한 값 : 20.0\n",
      "수식을 w로 미분한 값 : 22.0\n",
      "수식을 w로 미분한 값 : 24.0\n",
      "수식을 w로 미분한 값 : 26.0\n",
      "수식을 w로 미분한 값 : 28.0\n",
      "수식을 w로 미분한 값 : 30.0\n",
      "수식을 w로 미분한 값 : 32.0\n",
      "수식을 w로 미분한 값 : 34.0\n",
      "수식을 w로 미분한 값 : 36.0\n",
      "수식을 w로 미분한 값 : 38.0\n",
      "수식을 w로 미분한 값 : 40.0\n",
      "수식을 w로 미분한 값 : 42.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "  z = 2*w\n",
    "\n",
    "  z.backward()\n",
    "  print('수식을 w로 미분한 값 : {}'.format(w.grad))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계속해서 미분값인 2가 누적. 그래서 optimizer.zero_grad()를 통해 미분값을 계속 0으로 초기화."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level Implementation with `nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 linear regression 모델을 만들면 되는데, 기본적으로 PyTorch의 모든 모델은 제공되는 `nn.Module`을 inherit 해서 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    #__init__(): 객체가 갖는 속성값을 초기화하는 역할 -> 객체가 생성될 때 자동으로 호출\n",
    "    # super()호출 >>>> 오버라이딩,  nn.Module 클래스의 속성들을 가지고 초기화 \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "    \n",
    "    #forward() :모델이 학습데이터를 입력받아서 forward 연산을 진행 -> 객체를 데이터와 함께 호출하면 자동으로 실행\n",
    "    # forward 연산: H(x)  식에 입력 x로부터 예측된 y를 얻는 것\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 `__init__`에서는 사용할 레이어들을 정의. 여기서 우리는 linear regression 모델을 만들기 때문에, `nn.Linear` 를 이용. 그리고 `forward`에서는 이 모델이 어떻게 입력값에서 출력값을 계산하는지 나타냄."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.4283]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.9461], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "#객체 호출하기\n",
    "model = LinearRegressionModel()\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 값 모두 현재는 랜덤 초기화. 그리고 학습대상이므로 requires_grad=True가 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델을 생성해서 예측값 $H(x)$를 구해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3744],\n",
      "        [-1.8027],\n",
      "        [-2.2310]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 mean squared error (MSE) 로 cost를 구해보자. MSE 역시 PyTorch에서 기본적으로 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3744],\n",
      "        [-1.8027],\n",
      "        [-2.2310]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[2.],\n",
      "        [4.],\n",
      "        [6.]])\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = F.mse_loss(hypothesis, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(37.6027, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 주어진 cost를 이용해 $H(x)$ 의 $W, b$ 를 바꾸어서 cost를 줄인다. 이때 PyTorch의 `torch.optim` 에 있는 `optimizer` 들 중 하나를 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "cost.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.264, b: -0.740 Cost: 55.135620\n",
      "Epoch  100/1000 W: 2.866, b: 0.305 Cost: 0.013451\n",
      "Epoch  200/1000 W: 2.894, b: 0.240 Cost: 0.008312\n",
      "Epoch  300/1000 W: 2.917, b: 0.189 Cost: 0.005136\n",
      "Epoch  400/1000 W: 2.935, b: 0.148 Cost: 0.003174\n",
      "Epoch  500/1000 W: 2.949, b: 0.117 Cost: 0.001961\n",
      "Epoch  600/1000 W: 2.960, b: 0.092 Cost: 0.001212\n",
      "Epoch  700/1000 W: 2.968, b: 0.072 Cost: 0.000749\n",
      "Epoch  800/1000 W: 2.975, b: 0.057 Cost: 0.000463\n",
      "Epoch  900/1000 W: 2.980, b: 0.045 Cost: 0.000286\n",
      "Epoch 1000/1000 W: 2.985, b: 0.035 Cost: 0.000177\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[3], [6], [9]])\n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "점점 $H(x)$ 의 $W$ 와 $b$ 를 조정해서 cost가 줄어드는 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W와 b의 값도 최적화가 되었는지 확인. x에 임의의 값 4를 넣어 모델이 예측하는 y의 값을 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 4일 때의 예측값 : tensor([[11.9734]], grad_fn=<AddmmBackward0>)\n",
      "[Parameter containing:\n",
      "tensor([[2.9846]], requires_grad=True), Parameter containing:\n",
      "tensor([0.0350], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 4를 선언\n",
    "new_var =  torch.FloatTensor([[4.0]]) \n",
    "# 입력한 값 4에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var) # forward 연산\n",
    "# y = 2x 이므로 입력이 4라면 y가 8에 가까운 값이 나와야 제대로 학습이 된 것\n",
    "print(\"훈련 후 입력이 4일 때의 예측값 :\", pred_y) \n",
    "\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('science_dnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c0f2ae45ca5b13b9dba64e90d12e7a2d503c99b3d3d62896f22c1678fa3ef761"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
