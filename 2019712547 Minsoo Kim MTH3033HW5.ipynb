{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    #__init__(): 객체가 갖는 속성값을 초기화하는 역할 -> 객체가 생성될 때 자동으로 호출\n",
    "    # super()호출 >>>> 오버라이딩,  nn.Module 클래스의 속성들을 가지고 초기화 \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "    \n",
    "    #forward() :모델이 학습데이터를 입력받아서 forward 연산을 진행 -> 객체를 데이터와 함께 호출하면 자동으로 실행\n",
    "    # forward 연산: H(x)  식에 입력 x로부터 예측된 y를 얻는 것\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./test-score.csv', header=None)\n",
    "\n",
    "data_in = np.loadtxt(fname='./test-score.csv', delimiter=',')\n",
    "x_train = torch.from_numpy(data_in[:, 0:3]).float()             # (N, 3)\n",
    "y_train = torch.from_numpy(data_in[:, 3]).unsqueeze(1).float()  # (N, 1)\n",
    "\n",
    "# x_train = df.iloc[:, :3].to_numpy()\n",
    "# x_train = torch.from_numpy(x_train)\n",
    "# x_train = x_train.type(torch.float32)\n",
    "# # x_train.reshape(25, 3)\n",
    "# y_train = df.iloc[:, 3].to_numpy()\n",
    "# y_train = torch.from_numpy(y_train)\n",
    "# y_train = y_train.type(torch.float32)\n",
    "# # y_train.reshape(25, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 W: (0.268917, -0.08977151, 0.39426336), b: 0.151 Cost: 36952.648438\n",
      "Epoch  100/2000 W: (0.735717, 0.39160162, 0.8917355), b: 0.156 Cost: 8.927736\n",
      "Epoch  200/2000 W: (0.7262771, 0.39386705, 0.8986972), b: 0.156 Cost: 8.784955\n",
      "Epoch  300/2000 W: (0.71707296, 0.39612663, 0.90543556), b: 0.156 Cost: 8.649653\n",
      "Epoch  400/2000 W: (0.7080991, 0.39837795, 0.91195816), b: 0.156 Cost: 8.521446\n",
      "Epoch  500/2000 W: (0.6993494, 0.40061942, 0.91827255), b: 0.156 Cost: 8.399905\n",
      "Epoch  600/2000 W: (0.6908181, 0.40284938, 0.92438596), b: 0.156 Cost: 8.284714\n",
      "Epoch  700/2000 W: (0.6824997, 0.40506613, 0.9303053), b: 0.156 Cost: 8.175515\n",
      "Epoch  800/2000 W: (0.67438936, 0.40726817, 0.936037), b: 0.156 Cost: 8.071971\n",
      "Epoch  900/2000 W: (0.6664814, 0.40945405, 0.9415878), b: 0.155 Cost: 7.973793\n",
      "Epoch 1000/2000 W: (0.65877074, 0.4116226, 0.9469637), b: 0.155 Cost: 7.880671\n",
      "Epoch 1100/2000 W: (0.65125257, 0.41377255, 0.9521709), b: 0.155 Cost: 7.792363\n",
      "Epoch 1200/2000 W: (0.64392185, 0.41590318, 0.95721465), b: 0.155 Cost: 7.708580\n",
      "Epoch 1300/2000 W: (0.63677394, 0.41801327, 0.9621009), b: 0.155 Cost: 7.629111\n",
      "Epoch 1400/2000 W: (0.6298044, 0.4201017, 0.96683496), b: 0.155 Cost: 7.553720\n",
      "Epoch 1500/2000 W: (0.6230086, 0.42216784, 0.971422), b: 0.155 Cost: 7.482175\n",
      "Epoch 1600/2000 W: (0.616382, 0.42421108, 0.9758669), b: 0.155 Cost: 7.414281\n",
      "Epoch 1700/2000 W: (0.6099206, 0.42623082, 0.98017436), b: 0.154 Cost: 7.349861\n",
      "Epoch 1800/2000 W: (0.60362047, 0.42822608, 0.98434895), b: 0.154 Cost: 7.288705\n",
      "Epoch 1900/2000 W: (0.59747714, 0.43019673, 0.9883953), b: 0.154 Cost: 7.230670\n",
      "Epoch 2000/2000 W: (0.59148675, 0.43214232, 0.9923174), b: 0.154 Cost: 7.175561\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionModel()\n",
    "# W = torch.zeros(3, requires_grad=True)\n",
    "# b = torch.zeros(1, requires_grad=True)\n",
    "# Set up the optimizer\n",
    "# optimizer = optim.SGD([W, b], lr=1e-5)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-5)\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    # Compute H(x)\n",
    "    prediction = model(x_train)\n",
    "    # Compute the cost with MSE\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    # Update H(x)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    # print epoch and cost for every 20 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].detach().numpy()\n",
    "        b = params[1].detach().numpy()\n",
    "        s = 'Epoch {:4d}/{} W: {}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, tuple(*W), b.item(), cost.item())\n",
    "        print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[152.328 152.000 0.002]\n",
      " [185.476 185.000 0.003]\n",
      " [181.430 180.000 0.008]\n",
      " [198.519 196.000 0.013]\n",
      " [141.316 142.000 0.005]\n",
      " [105.959 101.000 0.049]\n",
      " [149.354 149.000 0.002]\n",
      " [111.693 115.000 0.029]\n",
      " [175.061 175.000 0.000]\n",
      " [164.455 164.000 0.003]\n",
      " [143.656 141.000 0.019]\n",
      " [143.079 141.000 0.015]\n",
      " [186.517 184.000 0.014]\n",
      " [153.892 152.000 0.012]\n",
      " [150.505 148.000 0.017]\n",
      " [188.885 192.000 0.016]\n",
      " [146.178 147.000 0.006]\n",
      " [179.243 183.000 0.021]\n",
      " [177.301 177.000 0.002]\n",
      " [158.566 159.000 0.003]\n",
      " [175.129 177.000 0.011]\n",
      " [174.774 175.000 0.001]\n",
      " [166.505 175.000 0.049]\n",
      " [151.429 149.000 0.016]\n",
      " [191.396 192.000 0.003]]\n"
     ]
    }
   ],
   "source": [
    "# model.eval()  # model.train()\n",
    "\n",
    "# Put the test input\n",
    "new_var = torch.FloatTensor([[73, 80, 75]])\n",
    "# Get the prediction value.\n",
    "pred_y = model(x_train).detach().numpy()\n",
    "real_y = y_train.numpy()\n",
    "err = np.abs(1.0 - pred_y / real_y)\n",
    "conc_y = np.hstack((pred_y, real_y, err))\n",
    "\n",
    "with np.printoptions(precision=3, formatter={'all': lambda x: x, 'float': lambda x: f\"{x:5.3f}\"}):\n",
    "    print(conc_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 53.,  46.,  55., 101.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in[5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('science_dnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0f2ae45ca5b13b9dba64e90d12e7a2d503c99b3d3d62896f22c1678fa3ef761"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
